{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 2: Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "### Shrenik Ganguli - CS23MTECH14014\n",
    "### Trishita Saha - CS23MTECH14016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk import bigrams, FreqDist, MLEProbDist, KneserNeyProbDist, ConditionalFreqDist, ConditionalProbDist\n",
    "from nltk.lm.models import MLE, KneserNeyInterpolated\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, flatten\n",
    "from nltk.lm.smoothing import KneserNey\n",
    "import random\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BOY WHO LIVED Mr and Mrs Dursley of number four Privet Drive were proud to say that they were perfectly normal thank you very much. They were the last people youd expect to be involved in anything strange or mysterious because they just didnt hold with such nonsense. Mr Dursley was the director of a firm called Grunnings which made drills. He was a big beefy man with hardly any neck although he did have a very large mustache. Mrs Dursley was thin and blonde and had nearly twice the usual amo\n"
     ]
    }
   ],
   "source": [
    "# Reading dataset\n",
    "with open('Datasets/Harry_Potter_all_books_preprocessed.txt', 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Since fullstops('.') are not at the correct position making changes to the data\n",
    "data = re.sub(r'\\s\\.([A-Z])', r'. \\1', data)\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing and tokenization of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(data)\n",
    "\n",
    "# Preprocess each sentence\n",
    "preprocessed_data = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_count = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Tokenize into words\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    word_count += len(words)\n",
    "    \n",
    "    if word_count > 10000:\n",
    "        break\n",
    "    \n",
    "    # Apply preprocessing steps\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Append the preprocessed words to the list\n",
    "    preprocessed_data.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fitting two bigram language models on the text: MLE and Kneser-Ney discounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 # for bigrams\n",
    "mle_train_data, mle_vocab = padded_everygram_pipeline(n, preprocessed_data)\n",
    "\n",
    "mle = MLE(n)\n",
    "mle.fit(mle_train_data, mle_vocab)\n",
    "\n",
    "kn_train_data, kn_vocab = padded_everygram_pipeline(n, preprocessed_data)\n",
    "\n",
    "# Create and train the Kneser-Ney interpolated model\n",
    "kn = KneserNeyInterpolated(n, discount=0.1)\n",
    "kn.fit(kn_train_data, kn_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the beginning words 1. ”Harry Potter” and 2. ”Dumbledore” to generate text using both the language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sequence generated with MLE and Harry Potter as beginning words is:\n",
      "harry potter boy name inside envelope hide horrible scar his face bad news vernon opened front\n",
      "\n",
      "\n",
      "The second sequence generated with MLE and Dumbledore as beginning word is:\n",
      "dumbledore she cant blame hed said motorcycle said aunt petunia back outside door mysterious people celebrating i trying disturb\n",
      "\n",
      "\n",
      "\n",
      "The first sequence generated with Kneser-Ney and Harry Potter as beginning words is:\n",
      "harry potter boy parking lot mind left headlight swelled roar looked though didnt improve mood tabby cat mirror\n",
      "\n",
      "\n",
      "The second sequence generated with Kneser-Ney and Dumbledore as beginning word is:\n",
      "dumbledore said dudley except stupid people way harry corner privet drive parent died much time\n"
     ]
    }
   ],
   "source": [
    "seed_phrase = ['Harry', 'Potter']\n",
    "seed_context = list(map(str.lower, seed_phrase))\n",
    "\n",
    "# Add the seed context to the beginning of the generated sequence\n",
    "generated_sequence = mle.generate(20, text_seed = seed_context, random_seed=7)\n",
    "generated_sequence = [word for word in generated_sequence if word not in ['<s>', '</s>']]\n",
    "full_generated_text = seed_context + generated_sequence\n",
    "print('The first sequence generated with MLE and Harry Potter as beginning words is:')\n",
    "print(' '.join(full_generated_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "seed_word = 'Dumbledore'\n",
    "seed_context2 = [seed_word.lower()]\n",
    "\n",
    "generated_sequence = mle.generate(20, text_seed = seed_context2, random_seed=42)\n",
    "generated_sequence = [word for word in generated_sequence if word not in ['<s>', '</s>']]\n",
    "full_generated_text = seed_context2 + generated_sequence\n",
    "print('The second sequence generated with MLE and Dumbledore as beginning word is:')\n",
    "print(' '.join(full_generated_text))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "generated_sequence = kn.generate(20, text_seed=seed_context, random_seed=7)\n",
    "generated_sequence = [word for word in generated_sequence if word not in ['<s>', '</s>']]\n",
    "full_generated_text = seed_context + generated_sequence\n",
    "print('The first sequence generated with Kneser-Ney and Harry Potter as beginning words is:')\n",
    "print(' '.join(full_generated_text))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "generated_sequence = kn.generate(20, text_seed=seed_context2, random_seed=42)\n",
    "generated_sequence = [word for word in generated_sequence if word not in ['<s>', '</s>']]\n",
    "full_generated_text = seed_context2 + generated_sequence\n",
    "print('The second sequence generated with Kneser-Ney and Dumbledore as beginning word is:')\n",
    "print(' '.join(full_generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1695 7\n"
     ]
    }
   ],
   "source": [
    "print(len(mle.vocab),mle.counts[['harry']]['potter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1695 7\n"
     ]
    }
   ],
   "source": [
    "print(len(kn.vocab),kn.counts[['harry']]['potter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Function to find the top k most probable words given some context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for the context \"['Harry', 'Potter']\": ['</s>', 'son', 'mr', 'arrived', 'small', 'away', 'thats', 'wasnt', 'if', 'involved']\n"
     ]
    }
   ],
   "source": [
    "def top_k_words(model, context, k):\n",
    "    \n",
    "    \n",
    "    #context = tuple(word.lower() for word in context if word.lower() in model.vocab)\n",
    "    context = (context[-1].lower(),) if context and context[-1].lower() in model.vocab else ()\n",
    "    \n",
    "    # Use model.vocab for vocabulary\n",
    "    vocab = set(model.vocab)\n",
    "    \n",
    "    # Filter out words not in the vocabulary\n",
    "    context = tuple(word for word in context if word in vocab)\n",
    "    \n",
    "    context = context\n",
    "\n",
    "    # Get the conditional probabilities for the next word\n",
    "    cond_prob_dist = model.context_counts(context)\n",
    "\n",
    "    # Sort the words based on their probabilities\n",
    "    sorted_words = sorted(cond_prob_dist, key=cond_prob_dist.get, reverse=True)\n",
    "    \n",
    "    return sorted_words[:k]\n",
    "\n",
    "beam_search_context = ['Harry','Potter',]\n",
    "top_k = 10\n",
    "top_words = top_k_words(mle, beam_search_context, top_k)\n",
    "print(f'Top {top_k} words for the context \"{beam_search_context}\": {top_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Implementing Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_top_k(model, start_context, k, depth):\n",
    "    # Initialize the beam with the start context\n",
    "    beam = [(start_context, 1.0)]\n",
    "\n",
    "    # Keep track of the top contexts during the iterations\n",
    "    top_contexts = []\n",
    "\n",
    "    for _ in range(depth):\n",
    "        new_beam = []\n",
    "\n",
    "        for context, score in beam:\n",
    "            # Generate all possible next words\n",
    "            next_words = set(model.vocab) - set(context)\n",
    "            #next_words = top_k_words(model, context, k)\n",
    "            #print(next_words)\n",
    "            for next_word in next_words:\n",
    "                # Calculate the score for the new context\n",
    "                new_context = context + (next_word,)\n",
    "                new_score = score * model.score(new_context[-1], new_context[:-1])\n",
    "\n",
    "                # Add the new context and score to the beam\n",
    "                new_beam.append((new_context, new_score))\n",
    "\n",
    "        # Sort the new beam based on scores in descending order\n",
    "        new_beam = sorted(new_beam, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Keep only the top k contexts\n",
    "        beam = new_beam[:k]\n",
    "\n",
    "        # Extend the top_contexts list with the current top k contexts\n",
    "        top_contexts.extend(beam)\n",
    "\n",
    "    # Sort the top contexts based on scores in descending order and return the top 5\n",
    "    top_contexts = sorted(top_contexts, key=lambda x: x[1], reverse=True)[:5]\n",
    "    return [context for context, _ in top_contexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Repeating part 3 using Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 sequences with MLE\n",
      "\n",
      "Sequence 1\n",
      "Dumbledore thinking \n",
      "Sequence 2\n",
      "Dumbledore uncle \n",
      "Sequence 3\n",
      "Dumbledore thinking uncle \n",
      "Sequence 4\n",
      "Dumbledore thinking swung \n",
      "Sequence 5\n",
      "Dumbledore thinking uncle swung \n",
      "\n",
      "Top 5 sequences with Kneser Ney\n",
      "\n",
      "Sequence 1\n",
      "Dumbledore </s> \n",
      "Sequence 2\n",
      "Dumbledore harry \n",
      "Sequence 3\n",
      "Dumbledore harry </s> \n",
      "Sequence 4\n",
      "Dumbledore </s> harry \n",
      "Sequence 5\n",
      "Dumbledore </s> harry potter "
     ]
    }
   ],
   "source": [
    "start_context = ('Dumbledore',)\n",
    "beam_width = 2\n",
    "search_depth = 10\n",
    "best_sequence_mle = beam_search_top_k(mle, start_context, beam_width, search_depth)\n",
    "best_sequence_kn = beam_search_top_k(kn, start_context, beam_width, search_depth)\n",
    "\n",
    "print(\"Top 5 sequences with MLE:\")\n",
    "for i, sequence in enumerate(best_sequence_mle):\n",
    "    print(f\"\\nSequence {i+1}\")\n",
    "    \n",
    "    for word in sequence:\n",
    "        print(word,end=\" \")\n",
    "        \n",
    "print(\"\\n\\nTop 5 sequences with Kneser Ney:\")\n",
    "for i, sequence in enumerate(best_sequence_kn):\n",
    "    print(f\"\\nSequence {i+1}\")\n",
    "    for word in sequence:\n",
    "        print(word,end=\" \")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
