{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8164340,"sourceType":"datasetVersion","datasetId":4830842}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Installing and importing the necessary packages**","metadata":{}},{"cell_type":"code","source":"pip install rouge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip show rouge","metadata":{"execution":{"iopub.status.busy":"2024-04-20T13:01:35.565903Z","iopub.execute_input":"2024-04-20T13:01:35.566293Z","iopub.status.idle":"2024-04-20T13:01:47.498661Z","shell.execute_reply.started":"2024-04-20T13:01:35.566253Z","shell.execute_reply":"2024-04-20T13:01:47.497519Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Name: rouge\nVersion: 1.0.1\nSummary: Full Python ROUGE Score Implementation (not a wrapper)\nHome-page: http://github.com/pltrdy/rouge\nAuthor: pltrdy\nAuthor-email: pltrdy@gmail.com\nLicense: LICENCE.txt\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: six\nRequired-by: \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install bert-score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip show bert-score","metadata":{"execution":{"iopub.status.busy":"2024-04-20T13:01:47.500950Z","iopub.execute_input":"2024-04-20T13:01:47.501296Z","iopub.status.idle":"2024-04-20T13:01:59.451687Z","shell.execute_reply.started":"2024-04-20T13:01:47.501265Z","shell.execute_reply":"2024-04-20T13:01:59.450562Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Name: bert-score\nVersion: 0.3.13\nSummary: PyTorch implementation of BERT score\nHome-page: https://github.com/Tiiiger/bert_score\nAuthor: Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi\nAuthor-email: tzhang@asapp.com\nLicense: MIT\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: matplotlib, numpy, packaging, pandas, requests, torch, tqdm, transformers\nRequired-by: \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install textstat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip show textstat","metadata":{"execution":{"iopub.status.busy":"2024-04-20T13:01:59.453100Z","iopub.execute_input":"2024-04-20T13:01:59.453386Z","iopub.status.idle":"2024-04-20T13:02:11.278331Z","shell.execute_reply.started":"2024-04-20T13:01:59.453361Z","shell.execute_reply":"2024-04-20T13:02:11.277095Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Name: textstat\nVersion: 0.7.3\nSummary: Calculate statistical features from text\nHome-page: https://github.com/shivam5992/textstat\nAuthor: Shivam Bansal, Chaitanya Aggarwal\nAuthor-email: shivam5992@gmail.com\nLicense: MIT\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: pyphen\nRequired-by: \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport glob\nfrom pathlib import Path\nfrom transformers import BartTokenizer, BartForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport shutil\nimport csv\nfrom rouge import Rouge\nimport bert_score\nfrom textstat import gunning_fog\nimport zipfile\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", message=\"Some non-default generation parameters are set in the model config\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", message=\"huggingface/tokenizers: The current process just got forked\", category=UserWarning)\n\n# warnings.resetwarnings()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:42:35.981826Z","iopub.execute_input":"2024-04-20T14:42:35.982567Z","iopub.status.idle":"2024-04-20T14:42:35.989464Z","shell.execute_reply.started":"2024-04-20T14:42:35.982532Z","shell.execute_reply":"2024-04-20T14:42:35.988468Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**GPU**","metadata":{}},{"cell_type":"code","source":"# Move tokenizer and model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Available Device is\",device)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T13:05:02.754909Z","iopub.execute_input":"2024-04-20T13:05:02.756145Z","iopub.status.idle":"2024-04-20T13:05:02.783082Z","shell.execute_reply.started":"2024-04-20T13:05:02.756110Z","shell.execute_reply":"2024-04-20T13:05:02.782161Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Available Device is cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Loading the models**","metadata":{}},{"cell_type":"code","source":"# Load Legal Pegasus tokenizer and model for summarization\nlegal_pegasus_tokenizer = AutoTokenizer.from_pretrained(\"nsi319/legal-pegasus\")\nlegal_pegasus_model = AutoModelForSeq2SeqLM.from_pretrained(\"nsi319/legal-pegasus\")\nlegal_pegasus_model = legal_pegasus_model.to(device)\n\n# Load BART tokenizer\nbart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n# Fine-tune BART model\nbart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\nbart_model.to(device)\n\nprint(legal_pegasus_model)\nprint(\"\\n\\n\")\nprint(bart_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T13:05:02.784769Z","iopub.execute_input":"2024-04-20T13:05:02.785076Z","iopub.status.idle":"2024-04-20T13:05:31.515501Z","shell.execute_reply.started":"2024-04-20T13:05:02.785025Z","shell.execute_reply":"2024-04-20T13:05:31.514536Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e6deaffc2a4be88e1e03e3c86b5d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b9b3fbf65a4e149aa7a7d4f38cd1d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4998fe383f3e46529768365780de5e1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d0cacdd5e14d93a358b41ff7d80e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"163afe8c98fe4ff7a3eb2a8bc09b2d01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"872f1ee3cd8d465184df7e852c734fae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba3a0de00ec4ac2b30f8f85fe05ac61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f1ef0e56f84aa7b2bc3bac4bc6317f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090ac0660359441d95eba12eac9650e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4c1df825174469d97dae97269b05b1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc211a30645c4f0483caa361e9236790"}},"metadata":{}},{"name":"stdout","text":"PegasusForConditionalGeneration(\n  (model): PegasusModel(\n    (shared): Embedding(96103, 1024, padding_idx=0)\n    (encoder): PegasusEncoder(\n      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n      (layers): ModuleList(\n        (0-15): 16 x PegasusEncoderLayer(\n          (self_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): PegasusDecoder(\n      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n      (layers): ModuleList(\n        (0-15): 16 x PegasusDecoderLayer(\n          (self_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): PegasusAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n)\n\n\n\nBartForConditionalGeneration(\n  (model): BartModel(\n    (shared): Embedding(50264, 1024, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"# Define a dataset class\nclass TextSimplificationDataset(Dataset):\n    def __init__(self, summaries_folder, simplify_folder, tokenizer):\n        self.summaries_folder = summaries_folder\n        self.simplify = self.load_texts(simplify_folder)\n        self.tokenizer = tokenizer\n    \n    def load_texts(self, folder):\n        texts = []\n        folder_path = Path(folder)\n        for file_path in folder_path.glob(\"*.txt\"):\n            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n                texts.append(file.read())\n        return texts\n    \n    def __len__(self):\n        return len(self.summaries_folder)\n    \n    def __getitem__(self, idx):\n        source_text = self.load_summary(idx)\n        target_text = self.simplify[idx]\n        encoding = self.tokenizer(source_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n        labels = self.tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")[\"input_ids\"]\n        return {k: v.squeeze(0) for k, v in encoding.items()}, labels.squeeze(0)\n    \n    def load_summary(self, idx):\n        file_path = self.summaries_folder[idx]\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n\n\n# Generate summaries for Judgement_Train folder\njudgement_folder = \"/kaggle/input/mildsum-split-2/Judgement_Train\"\njudgement_files = glob.glob(os.path.join(judgement_folder, \"*.txt\"))\n\nsummaries_folder = \"/kaggle/working/Summaries_Train\"\nos.makedirs(summaries_folder, exist_ok=True)\n\n# Set the model to evaluation mode for inference\nlegal_pegasus_model.eval()\n\nfor file_path in judgement_files:\n    file_name = os.path.basename(file_path)\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n        with torch.no_grad():\n            # Encode the input text\n            inputs = legal_pegasus_tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n            # Generate summary on GPU\n            summary = legal_pegasus_model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_length=100, num_beams=4, early_stopping=True)\n            # Save the summary with the same name as the input file\n            with open(os.path.join(summaries_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n                f.write(legal_pegasus_tokenizer.decode(summary[0], skip_special_tokens=True))\n\n\n\nprint(\"Training Started ...\")\n                \n# Move the input back to CPU\ninputs = inputs.to(\"cpu\")\n\n\n# Define paths to your dataset folders\nsimplify_folder = \"/kaggle/input/mildsum-split-2/Simplify_Train\"\n\n# Define dataset and dataloader\ndataset = TextSimplificationDataset(glob.glob(os.path.join(summaries_folder, \"*.txt\")), simplify_folder, bart_tokenizer)\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\noptimizer = torch.optim.AdamW(bart_model.parameters(), lr=5e-5)\n\n# Train the model\nbart_model.train()\nfor epoch in range(20):  # Adjust number of epochs as needed\n    for batch in dataloader:\n        inputs = {key: value.to(device) for key, value in batch[0].items()}\n        labels = batch[1].to(device)\n\n        outputs = bart_model(**inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    print(f\"|\\t Epoch {epoch+1} \\t | \\tLoss = {loss.item()} \\t|\")\n    print(\"-\" * 65)\n\n# Save the fine-tuned BART model\noutput_dir = \"/kaggle/working/fine_tuned_bart_model\"\nos.makedirs(output_dir, exist_ok=True)\nbart_tokenizer.save_pretrained(output_dir)\nbart_model.save_pretrained(output_dir)\nprint(\"\\nFine-tuned BART model saved successfully.\\n\")\n\nlabels = labels.to(\"cpu\")\n\nshutil.rmtree(summaries_folder)\nprint(\"\\nTraining Completed.\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-20T13:05:31.516975Z","iopub.execute_input":"2024-04-20T13:05:31.517317Z","iopub.status.idle":"2024-04-20T14:17:25.787624Z","shell.execute_reply.started":"2024-04-20T13:05:31.517293Z","shell.execute_reply":"2024-04-20T14:17:25.786654Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"|\t Epoch 1 \t | \tLoss = 2.6931285858154297 \t|\n-----------------------------------------------------------------\n|\t Epoch 2 \t | \tLoss = 2.2824175357818604 \t|\n-----------------------------------------------------------------\n|\t Epoch 3 \t | \tLoss = 1.8398253917694092 \t|\n-----------------------------------------------------------------\n|\t Epoch 4 \t | \tLoss = 1.863691806793213 \t|\n-----------------------------------------------------------------\n|\t Epoch 5 \t | \tLoss = 1.5864046812057495 \t|\n-----------------------------------------------------------------\n|\t Epoch 6 \t | \tLoss = 1.4915056228637695 \t|\n-----------------------------------------------------------------\n|\t Epoch 7 \t | \tLoss = 1.1613913774490356 \t|\n-----------------------------------------------------------------\n|\t Epoch 8 \t | \tLoss = 0.9879866242408752 \t|\n-----------------------------------------------------------------\n|\t Epoch 9 \t | \tLoss = 0.8358415961265564 \t|\n-----------------------------------------------------------------\n|\t Epoch 10 \t | \tLoss = 0.725231945514679 \t|\n-----------------------------------------------------------------\n|\t Epoch 11 \t | \tLoss = 0.5348801016807556 \t|\n-----------------------------------------------------------------\n|\t Epoch 12 \t | \tLoss = 0.4153452217578888 \t|\n-----------------------------------------------------------------\n|\t Epoch 13 \t | \tLoss = 0.29451337456703186 \t|\n-----------------------------------------------------------------\n|\t Epoch 14 \t | \tLoss = 0.2590201497077942 \t|\n-----------------------------------------------------------------\n|\t Epoch 15 \t | \tLoss = 0.21908250451087952 \t|\n-----------------------------------------------------------------\n|\t Epoch 16 \t | \tLoss = 0.18080326914787292 \t|\n-----------------------------------------------------------------\n|\t Epoch 17 \t | \tLoss = 0.13912950456142426 \t|\n-----------------------------------------------------------------\n|\t Epoch 18 \t | \tLoss = 0.14132118225097656 \t|\n-----------------------------------------------------------------\n|\t Epoch 19 \t | \tLoss = 0.12253470718860626 \t|\n-----------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"|\t Epoch 20 \t | \tLoss = 0.09266327321529388 \t|\n-----------------------------------------------------------------\n\nFine-tuned BART model saved successfully.\n\n\nTraining Completed.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Testing**","metadata":{}},{"cell_type":"code","source":"# Define paths to test dataset folders\njudgement_test_folder = \"/kaggle/input/mildsum-split-2/Judgement_Test\"\nsimplify_test_folder = \"/kaggle/input/mildsum-split-2/Simplify_Test\"\n\n# Generate summaries for Judgement_Test folder using Legal Pegasus model\ntest_summaries_folder = \"/kaggle/working/Summaries_Test\"\nos.makedirs(test_summaries_folder, exist_ok=True)\n\n# Set the Legal Pegasus model to evaluation mode for inference\nlegal_pegasus_model.eval()\n\nfor file_path in glob.glob(os.path.join(judgement_test_folder, \"*.txt\")):\n    file_name = os.path.basename(file_path)\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n        with torch.no_grad():\n            # Encode the input text\n            inputs = legal_pegasus_tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n            # Generate summary on GPU\n            summary = legal_pegasus_model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_length=100, num_beams=4, early_stopping=True)\n            # Save the summary with the same name as the input file\n            with open(os.path.join(test_summaries_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n                f.write(legal_pegasus_tokenizer.decode(summary[0], skip_special_tokens=True))\n                \n                \n# Define dataset and data loader for testing\ntest_dataset = TextSimplificationDataset(glob.glob(os.path.join(test_summaries_folder, \"*.txt\")), simplify_test_folder, bart_tokenizer)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n\n# Evaluate the fine-tuned BART model on the test dataset\nbart_model.eval()\ntotal_loss = 0.0\nnum_samples = 0\n\n# Calculatimg loss\nfor batch in test_dataloader:\n    \n    inputs = {key: value.to(device) for key, value in batch[0].items()}\n    labels = batch[1].to(device)\n\n    with torch.no_grad():\n        \n        outputs = bart_model(**inputs, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item() * labels.size(0)\n        num_samples += labels.size(0)\n\ntest_summary_files = glob.glob(os.path.join(test_summaries_folder, \"*.txt\"))\n\ngenerated_simplified_folder = \"/kaggle/working/Generated_Simplified\"\nos.makedirs(generated_simplified_folder, exist_ok=True)\n\n\n# Saving the generated summaries\nfor file_path in test_summary_files:\n    file_name = os.path.basename(file_path)\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n        with torch.no_grad():\n            # Encode the input text\n            inputs = bart_tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n            # Generate summary on GPU\n            simplified = bart_model.generate(inputs.input_ids, attention_mask=inputs.attention_mask, max_length=1024, num_beams=4, early_stopping=True)\n            # Save the summary with the same name as the input file\n            with open(os.path.join(generated_simplified_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n                f.write(bart_tokenizer.decode(simplified[0], skip_special_tokens=True))\n\n# Delete the test summaries folder\nshutil.rmtree(test_summaries_folder)\n                \naverage_loss = total_loss / num_samples\nprint(f\"Average Loss on Test Dataset: {average_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:17:25.789863Z","iopub.execute_input":"2024-04-20T14:17:25.790186Z","iopub.status.idle":"2024-04-20T14:38:51.405635Z","shell.execute_reply.started":"2024-04-20T14:17:25.790161Z","shell.execute_reply":"2024-04-20T14:38:51.404308Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Average Loss on Test Dataset: 4.346084743738174\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Metrics Calculation**","metadata":{}},{"cell_type":"code","source":"# Function to calculate ROUGE scores\ndef calculate_rouge(reference, candidate):\n    rouge = Rouge()\n    scores = rouge.get_scores(candidate, reference)\n    return scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f'], scores[0]['rouge-l']['f']\n\n# Function to calculate BERT score\ndef calculate_bert_score(reference, candidate):\n    _, _, f1 = bert_score.score([candidate], [reference], lang='en', model_type='bert-base-uncased', rescale_with_baseline=True)\n    return f1.item()\n\n# Paths to the folders\ntest_folder = \"/kaggle/input/mildsum-split-2/Simplify_Test\"\noutput_folder = \"/kaggle/working/Generated_Simplified\"\n\n# Get list of files in the folders\ntest_files = sorted(os.listdir(test_folder))\noutput_files = sorted(os.listdir(output_folder))\n\n# File path for scores.csv\ncsv_file_path = '/kaggle/working/rouge_scores.csv'\n\n# Initialize CSV writer\nwith open(csv_file_path, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['ID', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BERT Score'])\n\n    # Loop through files and calculate scores\n    for i, (test_file, output_file) in enumerate(zip(test_files, output_files)):\n        # Read content of files\n        with open(os.path.join(test_folder, test_file), 'r', encoding='utf-8') as f:\n            test_content = f.read()\n        with open(os.path.join(output_folder, output_file), 'r', encoding='utf-8') as f:\n            output_content = f.read()\n\n        # Calculate scores\n        rouge_1, rouge_2, rouge_l = calculate_rouge(test_content, output_content)\n        bert_score_val = calculate_bert_score(test_content, output_content)\n\n        # Write scores to CSV\n        writer.writerow([i+1, rouge_1, rouge_2, rouge_l, bert_score_val])\n\nprint(\"Scores calculated and saved to scores.csv.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:38:51.407184Z","iopub.execute_input":"2024-04-20T14:38:51.407623Z","iopub.status.idle":"2024-04-20T14:41:19.006241Z","shell.execute_reply.started":"2024-04-20T14:38:51.407594Z","shell.execute_reply":"2024-04-20T14:41:19.005106Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c076937a5643472bab164b4fe5d2cba7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a36b172dd2943b3ad9420323ed38dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b09d9ca69d48e49419f748c7ab300c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbcf594feb2043139e61e1b040fd1b20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec32a65f97ae43a7bb90a2b896fa45e8"}},"metadata":{}},{"name":"stdout","text":"Scores calculated and saved to scores.csv.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to calculate Gunning Fox Index (GFI) score\ndef calculate_gfi(text):\n    return gunning_fog(text)\n\n# Paths to the folders\ninput_folder = \"/kaggle/working/Generated_Simplified\"\n\n# Get list of files in the folder\ninput_files = sorted(os.listdir(input_folder))\n\n# Initialize CSV writer\nwith open('gfi_scores.csv', mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['ID', 'GFI Score'])\n\n    # Loop through files and calculate GFI scores\n    for i, input_file in enumerate(input_files):\n        # Read content of file\n        with open(os.path.join(input_folder, input_file), 'r', encoding='utf-8') as f:\n            content = f.read()\n\n        # Calculate GFI score\n        gfi_score = calculate_gfi(content)\n\n        # Write scores to CSV\n        writer.writerow([i+1, gfi_score])\n\nprint(\"GFI scores calculated and saved to gfi_scores.csv.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:41:19.007649Z","iopub.execute_input":"2024-04-20T14:41:19.007967Z","iopub.status.idle":"2024-04-20T14:41:19.626868Z","shell.execute_reply.started":"2024-04-20T14:41:19.007940Z","shell.execute_reply":"2024-04-20T14:41:19.625938Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"GFI scores calculated and saved to gfi_scores.csv.\n","output_type":"stream"}]},{"cell_type":"code","source":"directory_to_zip = '/kaggle/working/Generated_Simplified'\noutput_zip_file = '/kaggle/working/Generated_Simplified.zip'\n\nwith zipfile.ZipFile(output_zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(directory_to_zip):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), directory_to_zip))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:43:25.557874Z","iopub.execute_input":"2024-04-20T14:43:25.558650Z","iopub.status.idle":"2024-04-20T14:43:25.603683Z","shell.execute_reply.started":"2024-04-20T14:43:25.558614Z","shell.execute_reply":"2024-04-20T14:43:25.602697Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Remove directories (if needed)**","metadata":{}},{"cell_type":"code","source":"# !rm -r /kaggle/working/fine_tuned_bart_model\n# !rm -r /kaggle/working/Generated_Simplified\n# !rm -r /kaggle/working/rouge_scores.csv\n# !rm -r /kaggle/working/gfi_scores.csv\n# !rm -r /kaggle/working/Summaries_Train\n# !rm -r /kaggle/working/Summaries_Test","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:41:19.628111Z","iopub.execute_input":"2024-04-20T14:41:19.628416Z","iopub.status.idle":"2024-04-20T14:41:19.632247Z","shell.execute_reply.started":"2024-04-20T14:41:19.628391Z","shell.execute_reply":"2024-04-20T14:41:19.631387Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}